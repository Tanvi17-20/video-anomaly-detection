{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d37c6b0",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 1: Import modules"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import zipfile\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator9\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt  # For displaying images if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3135c2ff",
   "metadata": {
    "title": "Cell 2: Extract ZIP file (Assuming crime.zip is in your project directory)"
   },
   "outputs": [],
   "source": [
    "# zip_file_name = \"crime.zip\"  # Ensure this file is in your working directory\n",
    "# with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"./\")  # Extract in the current directory\n",
    "\n",
    "# print(\"Dataset extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt  # For displaying images if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e101ef49",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Cell 3: Data augmentation setup"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True, zoom_range=0.2)\n",
    "\n",
    "def augment_images(folder, target_count):\n",
    "    all_files = os.listdir(folder)\n",
    "    img_list = [cv2.imread(os.path.join(folder, file)) for file in all_files]\n",
    "\n",
    "    img_list = np.array([cv2.resize(img, (64, 64)) for img in img_list if img is not None])\n",
    "    img_list = img_list / 255.0  # Normalize\n",
    "\n",
    "    i = 0\n",
    "    for img in datagen.flow(img_list, batch_size=1, save_to_dir=folder, save_prefix=\"aug\", save_format=\"png\"):\n",
    "        i += 1\n",
    "        if len(os.listdir(folder)) >= target_count:\n",
    "            break\n",
    "        \n",
    "        # Calculate the number of images in each folder outside of the function\n",
    "        anomaly_count = len(os.listdir(\"crime/Anomaly\"))\n",
    "        normal_count = len(os.listdir(\"crime/Normal\"))\n",
    "        max_count = max(anomaly_count, normal_count)\n",
    "\n",
    "        augment_images(\"crime/Anomaly\", max_count)\n",
    "        augment_images(\"crime/Normal\", max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d848f2ce",
   "metadata": {
    "title": "Cell 4: Augment images in folders"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1889 793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(os.listdir(\"crime/Anomaly\")), len(os.listdir(\"crime/Normal\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4e45a8",
   "metadata": {
    "title": "Cell 5: Load and preprocess images"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, IMG_SIZE)\n",
    "            img = img / 255.0  # Normalize\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "anomaly_path = 'crime/Anomaly'\n",
    "normal_path = 'crime/Normal'\n",
    "anomaly_images, anomaly_labels = load_images_from_folder(anomaly_path, 1)\n",
    "normal_images, normal_labels = load_images_from_folder(normal_path, 0)\n",
    "\n",
    "X = np.array(anomaly_images + normal_images)\n",
    "y = np.array(anomaly_labels + normal_labels)\n",
    "\n",
    "# Reduce dataset size to avoid memory error (optional: change 10000 to 5000 or smaller)\n",
    "# X = X[:2000]\n",
    "# y = y[:2000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559469d8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Cell 6: Build and train the CNN model"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anomoly Detection\\VS\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.7303 - loss: 0.5402 - val_accuracy: 1.0000 - val_loss: 8.0133e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.4735e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 1.2605e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 0.9977 - loss: 0.0039 - val_accuracy: 0.9069 - val_loss: 0.1266\n",
      "Epoch 5/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 0.9687 - loss: 0.1094 - val_accuracy: 1.0000 - val_loss: 1.1143e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - accuracy: 0.9993 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.4617e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.1011e-04 - val_accuracy: 1.0000 - val_loss: 3.3857e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 4.1280e-05 - val_accuracy: 1.0000 - val_loss: 2.1364e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 6.3963e-05 - val_accuracy: 1.0000 - val_loss: 4.6303e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 6.0055e-05 - val_accuracy: 1.0000 - val_loss: 1.6723e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 9.6277e-04 - val_accuracy: 1.0000 - val_loss: 2.5602e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 1.0019e-04 - val_accuracy: 1.0000 - val_loss: 8.8308e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 2.3924e-05 - val_accuracy: 1.0000 - val_loss: 3.4739e-06\n",
      "Epoch 14/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 8.1201e-06 - val_accuracy: 1.0000 - val_loss: 1.7489e-06\n",
      "Epoch 15/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 4.1309e-06 - val_accuracy: 1.0000 - val_loss: 1.3495e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 3.3359e-06 - val_accuracy: 1.0000 - val_loss: 8.6656e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 1.0787e-06 - val_accuracy: 1.0000 - val_loss: 6.7259e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 2.6246e-06 - val_accuracy: 1.0000 - val_loss: 4.3682e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 5.6931e-06 - val_accuracy: 1.0000 - val_loss: 3.2857e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 9.0035e-07 - val_accuracy: 1.0000 - val_loss: 2.7392e-07\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "model.save('anomaly_detector.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afbd53de",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Cell 7: Define prediction function for video frames"
   },
   "outputs": [],
   "source": [
    "def predict_and_display_frame(video_path, model, frame_interval=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    first_frame = None\n",
    "    detected_anomaly = False\n",
    "    output_frame = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            if first_frame is None:\n",
    "                first_frame = frame.copy()\n",
    "\n",
    "            frame_resized = cv2.resize(frame, IMG_SIZE)\n",
    "            frame_norm = frame_resized / 255.0\n",
    "            input_frame = np.expand_dims(frame_norm, axis=0)\n",
    "            prediction = model.predict(input_frame)[0][0]\n",
    "\n",
    "            if prediction > 0.5:\n",
    "                output_frame = frame.copy()\n",
    "                cv2.putText(output_frame, \"Anomaly\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                detected_anomaly = True\n",
    "                break\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not detected_anomaly:\n",
    "        if first_frame is not None:\n",
    "            output_frame = first_frame.copy()\n",
    "            cv2.putText(output_frame, \"Normal\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            print(\"No frames processed. Check the video file.\")\n",
    "            return\n",
    "\n",
    "    # Display the frame. For a GUI window:\n",
    "    cv2.imshow(\"Output\", output_frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    # Alternatively, to display inline (e.g., in a Jupyter Notebook):\n",
    "    # plt.imshow(cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB))\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8351171e",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell 8: Video processing examples"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames in video: 1967\n"
     ]
    }
   ],
   "source": [
    "video_path1 = \"crime/Shoplifting005_x264.mp4\"\n",
    "cap = cv2.VideoCapture(video_path1)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames in video: {total_frames}\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0f8bcf",
   "metadata": {
    "lines_to_next_cell": 3,
    "title": "Cell 9: Output"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n"
     ]
    }
   ],
   "source": [
    "video_path2 = \"crime/Shoplifting009_x264.mp4\"\n",
    "predict_and_display_frame(video_path2, model)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
